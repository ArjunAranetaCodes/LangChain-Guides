{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Text summarization plays a crucial role in handling large volumes of text data, enabling users to quickly grasp essential information without reading lengthy documents. With advancements in deep learning and natural language processing (NLP), we now have powerful tools and techniques at our disposal to build efficient text summarization systems. One such tool is the TransferTransformer (T5) developed by Google Research.\n",
        "\n",
        "This tutorial focuses on implementing extractive text summarization using the T5 model available through Hugging Face's popular Transformers library. We'll walk through the necessary steps, including setting up the environment, preparing input data, generating summaries, and interpreting results.\n",
        "\n",
        "Let's dive into the code and discuss it line by line:"
      ],
      "metadata": {
        "id": "0Ls5nmsx8XA9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers"
      ],
      "metadata": {
        "id": "jJ1zuEoYQlR6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Installs the latest version of the Transformers library created by Hugging Face. It contains over 100+ pre-trained models supporting tasks ranging from question answering, text generation, translation, etc."
      ],
      "metadata": {
        "id": "pk-d0nJD8b20"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZYEpu73ZQibO"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoTokenizer, TFT5ForConditionalGeneration"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Imports two classes required for summarization - AutoTokenizer and TFT5ForConditionalGeneration.\n",
        "\n",
        "**AutoTokenizer** handles converting raw text strings into numerical representations called tensors.\n",
        "\n",
        "**TFT5ForConditionalGeneration** represents the fine-tuned T5 model specifically designed for conditional generation tasks like summarization."
      ],
      "metadata": {
        "id": "4GOWturB8k9V"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = AutoTokenizer.from_pretrained(\"google-t5/t5-small\")"
      ],
      "metadata": {
        "id": "mmZf4Mj08dSO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Initializes the tokenizer object with the 'google-t5/t5-small' checkpoint which includes both the vocabulary and encoding rules used during training."
      ],
      "metadata": {
        "id": "0hO5muy38oP1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = TFT5ForConditionalGeneration.from_pretrained(\"google-t5/t5-small\")"
      ],
      "metadata": {
        "id": "DVTYeSQB8r02"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Loads the fine-tuned T5 model weights trained for summarization task from the 'google-t5/t5-small' checkpoint."
      ],
      "metadata": {
        "id": "jvcOAjNR8q0X"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "text = \"\"\"summarize: Hugging Face: Revolutionizing Natural Language Processing Introduction In the rapidly evolving field of Natural Language Processing (NLP),\n",
        "Hugging Face has emerged as a prominent and innovative force. This article will explore the story and significance of Hugging Face, a company that\n",
        "has made remarkable contributions to NLP and AI as a whole. From its inception to its role in democratizing AI, Hugging Face has left an\n",
        "indelible mark on the industry.  The Birth of Hugging Face Hugging Face was founded in 2016 by Cl√©ment Delangue, Julien Chaumond, and Thomas Wolf.\n",
        "The name Hugging Face was chosen to reflect the company's mission of making AI models more accessible and friendly to humans, much like a\n",
        "comforting hug. Initially, they began as a chatbot company but later shifted their focus to NLP, driven by their belief in\n",
        "the transformative potential of this technology. Transformative Innovations Hugging Face is best known for its open-source contributions,\n",
        "particularly the Transformers library. This library has become the de facto standard for NLP and enables researchers, developers,\n",
        "and organizations to easily access and utilize state-of-the-art pre-trained language models, such as BERT, GPT-3, and more.\n",
        "These models have countless applications, from chatbots and virtual assistants to language translation and sentiment analysis. \"\"\"\n"
      ],
      "metadata": {
        "id": "AZypehN5QkJO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Defines a long string containing text describing Hugging Face and its achievements, suitable for summarization."
      ],
      "metadata": {
        "id": "we0yIZq28uAz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "inputs = tokenizer(text, return_tensors=\"tf\").input_ids  # Batch size 1"
      ],
      "metadata": {
        "id": "saCG3e0A8ezA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Prepares input data for feeding into the model by tokenizing the given text and returning TensorFlow compatible tensor IDs."
      ],
      "metadata": {
        "id": "kGgD3lQB8vsO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "outputs = model.generate(inputs)"
      ],
      "metadata": {
        "id": "MhRPWDuX8iDo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Performs actual summary generation based on the prepared input tensor. This function returns the most likely sequence of tokens representing the summary."
      ],
      "metadata": {
        "id": "9CBkA4ED8xBN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(tokenizer.decode(outputs[0], skip_special_tokens=True))"
      ],
      "metadata": {
        "id": "Fluu1wdD8i60"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "s3iKbec-8z-f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Decodes the generated output tensor back into human readable format removing special characters added during tokenization."
      ],
      "metadata": {
        "id": "6gn2natL8yAX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "By following these lines of codes, one can create a simple yet effective summarization system utilizing the power of transfer learning offered by Hugging Face's T5 model."
      ],
      "metadata": {
        "id": "Kn5lYJZn8zpq"
      }
    }
  ]
}