{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Table question answering (TQA) is a task in natural language processing that involves extracting answers from structured data such as tables based on user queries. This can be useful for applications like virtual assistants or chatbots that need to answer questions about tabular data. In this tutorial, we will use the Hugging Face Transformer's Tapex model and tokenizer to perform TQA on a simple table containing information about different Olympic games."
      ],
      "metadata": {
        "id": "LNlJU2QZM0wG"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RhspHUhFKe4b"
      },
      "outputs": [],
      "source": [
        "!pip install transformers pandas"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This command installs two python libraries - transformers and pandas which are required for this tutorial. The first one provides access to pre-trained models like BART, RoBERTa etc., while the second one helps us manage our tabular data."
      ],
      "metadata": {
        "id": "iXH8oV5hNM2U"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import TapexTokenizer, BartForConditionalGeneration\n",
        "import pandas as pd"
      ],
      "metadata": {
        "id": "Y8e8lIyaNRR-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We import necessary classes/functions from their respective libraries. Here, we're going to use **TapexTokenizer** and **BartForConditionalGeneration** from transformers and DataFrame from pandas."
      ],
      "metadata": {
        "id": "EuNSkBf_NRm5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = TapexTokenizer.from_pretrained(\"microsoft/tapex-base\")\n",
        "model = BartForConditionalGeneration.from_pretrained(\"microsoft/tapex-base\")"
      ],
      "metadata": {
        "id": "t6d2iZbcNbDC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "These lines initialize the tokenizer and the model provided by Microsoft under 'tapex-base'. A tokenizer breaks down sentences into tokens (words, punctuation symbols, etc.) understandable by machine learning algorithms. On the other hand, a model takes these tokens and generates responses."
      ],
      "metadata": {
        "id": "oiXJDDrNNbSM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data = {\n",
        "    \"year\": [1896, 1900, 1904, 2004, 2008, 2012],\n",
        "    \"city\": [\"athens\", \"paris\", \"st. louis\", \"athens\", \"beijing\", \"london\"]\n",
        "}\n",
        "table = pd.DataFrame.from_dict(data)\n"
      ],
      "metadata": {
        "id": "bzCGSaAUKyYE"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This part creates a dictionary with key value pairs representing columns (\"headers\") and corresponding values. We then convert it into a pandas DataFrame object called 'table'."
      ],
      "metadata": {
        "id": "DYPxGQbCNe79"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "query = \"select year where city = athens\"\n",
        "encoding = tokenizer(table=table, query=query, return_tensors=\"pt\")"
      ],
      "metadata": {
        "id": "ZRNm56QeNjGz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here, we define our SQL-like query string. Then, we prepare input for our model by encoding our table & query using our loaded tokenizer. It returns tensor format inputs expected by PyTorch models."
      ],
      "metadata": {
        "id": "8eJ9A4u9NjTx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "outputs = model.generate(**encoding)"
      ],
      "metadata": {
        "id": "iWFhFusSNnpJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The generate method runs the actual prediction process given encoded inputs."
      ],
      "metadata": {
        "id": "SUmjNcS5Nn0F"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(tokenizer.batch_decode(outputs, skip_special_tokens=True))\n",
        "#2004"
      ],
      "metadata": {
        "id": "eNLLb2dVLWVW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Finally, we decode output tensors back to human readable text form. Our result shows that the correct year associated with Athens Olympics was returned successfully!"
      ],
      "metadata": {
        "id": "vgtOQtasNqVt"
      }
    }
  ]
}