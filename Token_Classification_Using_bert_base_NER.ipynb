{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "**Token classification**, also known as sequence labeling or token labeling, is a fundamental task in Natural Language Processing (NLP) where individual words or subwords in a given text are assigned labels based on their contextual meaning or role within the broader linguistic structure. Label sets vary depending on the application, ranging from Part-of-Speech (PoS) tagging, chunking, and named entity recognition (NER) to semantic roles and syntactic dependencies identification. For instance, in NER, tokens may receive labels such as 'PERSON', 'LOCATION', or 'ORGANIZATION'.\n",
        "\n",
        "Accurate token classification plays a crucial role in numerous downstream applications, enabling better understanding and extraction of relevant information from unstructured data sources, thereby enhancing overall performance across various NLP systems and applications. Leveraging transfer learning via pre-trained models has significantly improved token classification accuracy while reducing computational costs compared to training models from scratch, making it accessible even to resource-constrained settings."
      ],
      "metadata": {
        "id": "lieh0o29KUr1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Hugging Face Model Link**:\n",
        "https://huggingface.co/dslim/bert-base-NER"
      ],
      "metadata": {
        "id": "jQm7gVqlHemE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Model Description**:\n",
        "\n",
        "bert-base-NER is a fine-tuned BERT model that is ready to use for Named Entity Recognition and achieves state-of-the-art performance for the NER task. It has been trained to recognize four types of entities: location (LOC), organizations (ORG), person (PER) and Miscellaneous (MISC)."
      ],
      "metadata": {
        "id": "GLxU2_KyHiGw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The first step is to install the transformers package developed by Hugging Face, which provides pre-trained models and tools for natural language processing tasks such as token classification. The exclamation point at the beginning of the command allows us to run shell commands within our Jupyter notebook or Colab environment."
      ],
      "metadata": {
        "id": "gE51ZVcGJhbS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6l_Qg8KAH2CK",
        "outputId": "d85ca7bb-07ca-44aa-c1e5-e00c82500afe"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.38.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.13.1)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.19.3 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.20.3)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.25.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (24.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2023.12.25)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.31.0)\n",
            "Requirement already satisfied: tokenizers<0.19,>=0.14 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.15.2)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.2)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.2)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.19.3->transformers) (2023.6.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.19.3->transformers) (4.10.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2024.2.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Next, we will import the necessary classes from the transformers module:"
      ],
      "metadata": {
        "id": "xu6C_Y-3Jk8g"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer, AutoModelForTokenClassification\n",
        "from transformers import pipeline"
      ],
      "metadata": {
        "id": "kmPn_rYXJloi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**AutoTokenizer** is used to encode text input into numerical tokens that can be fed into a BERT-based model. In this example, we use the pre-trained NER (Named Entity Recognition) tokenizer from the dslim/bert-base-NER checkpoint.\n",
        "\n",
        "**AutoModelForTokenClassification** is a class representing fine-tuned BERT models specifically designed for sequence tagging tasks like named entity recognition. We load the same pre-trained NER model from the dslim/bert-base-NER checkpoint.\n",
        "\n",
        "**pipeline** creates a user-friendly interface around various NLP pipelines, including Named Entity Recognition (NER). It abstracts away many complexities related to handling raw model outputs and converts them into more interpretable formats.\n",
        "\n",
        "After importing the required modules, let's create instances of these classes:"
      ],
      "metadata": {
        "id": "d6rmi64dJpaX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = AutoTokenizer.from_pretrained(\"dslim/bert-base-NER\")\n",
        "model = AutoModelForTokenClassification.from_pretrained(\"dslim/bert-base-NER\")"
      ],
      "metadata": {
        "id": "kBBIEZacJzaf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We initialize two objects - one for the tokenizer and another for the model. These are loaded directly from the previously mentioned dslim/bert-base-NER checkpoint. This specific checkpoint was trained for recognizing entities in Chinese texts but should still work reasonably well for other languages due to its generalized nature.\n",
        "\n",
        "Now, it's time to set up the NLP pipeline for Named Entity Recognition:"
      ],
      "metadata": {
        "id": "6ePnNOrzJz5P"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "nlp = pipeline(\"ner\", model=model, tokenizer=tokenizer)"
      ],
      "metadata": {
        "id": "TN4JFgUyJ4Bz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here, we define the NLP pipeline for Named Entity Recognition (NER), passing our custom model and tokenizer instances as arguments. Once defined, the nlp object takes care of formatting the inputs correctly before feeding them to the underlying BERT model and interpreting its output accordingly.\n",
        "\n",
        "Finally, let's test out our NER system:"
      ],
      "metadata": {
        "id": "uAVaNYiWJ4eR"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "5f-uKk3RHazb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 193
        },
        "outputId": "0432c4d8-928f-4abd-e4f7-ac6befb6959b"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'nlp' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-9616aa341b39>\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mexample\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"My name is Wolfgang and I live in Berlin\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mner_results\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnlp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexample\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mner_results\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'nlp' is not defined"
          ]
        }
      ],
      "source": [
        "\n",
        "example = \"My name is Wolfgang and I live in Berlin\"\n",
        "\n",
        "ner_results = nlp(example)\n",
        "print(ner_results)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Our last piece of code defines an example sentence containing multiple entities (\"Wolfgang\" being a person's name and \"Berlin\" being a location). We then pass this string through the nlp pipeline, obtaining labeled entities as results. Running this block prints the recognized entities along with their respective categories."
      ],
      "metadata": {
        "id": "QbLYqeMZJ9pf"
      }
    }
  ]
}